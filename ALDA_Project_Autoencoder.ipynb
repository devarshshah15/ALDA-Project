{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALDA_Project_Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoFICZjrecHM",
        "outputId": "58590325-da9c-4750-8b7c-3d9b0f0629cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K1-SYiZfi28"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import datetime\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abp6QUsjgJAO"
      },
      "source": [
        "# Without softmax\n",
        "class AutoEncoder(nn.Module):\n",
        "  def __init__(self,dropOutProb = 0.4,**kwargs):\n",
        "    super(AutoEncoder,self).__init__()\n",
        "    self.encoder_hidden_layer1 = nn.Linear(kwargs[\"input_shape\"],1024)\n",
        "    self.encoder_hidden_layer2 = nn.Linear(1024,512)\n",
        "    self.encoder_hidden_layer3 = nn.Linear(512,512)\n",
        "    self.representation_layer = nn.Linear(512,256)\n",
        "    self.decoder_hidden_layer1 = nn.Linear(256,512)\n",
        "    self.decoder_hidden_layer2 = nn.Linear(512,512)\n",
        "    self.decoder_hidden_layer3 = nn.Linear(512,1024)\n",
        "    self.decoder_output_layer = nn.Linear(1024,kwargs[\"input_shape\"])\n",
        "    self.Dropout = nn.Dropout(dropOutProb)\n",
        "\n",
        "  def forward(self,features):\n",
        "    encoder1 = nn.functional.relu(self.encoder_hidden_layer1(features))\n",
        "    encoder1 = self.Dropout(encoder1)\n",
        "    encoder2 = nn.functional.relu(self.encoder_hidden_layer2(encoder1))\n",
        "    encoder2 = self.Dropout(encoder2)\n",
        "    encoder3 = nn.functional.relu(self.encoder_hidden_layer3(encoder2))\n",
        "    encoder3 = self.Dropout(encoder3)\n",
        "    representation = nn.functional.relu(self.representation_layer(encoder3))\n",
        "    decoder1 = nn.functional.relu(self.decoder_hidden_layer1(representation))\n",
        "    decoder2 = nn.functional.relu(self.decoder_hidden_layer2(decoder1))\n",
        "    decoder3 = nn.functional.relu(self.decoder_hidden_layer3(decoder2))\n",
        "    output = self.decoder_output_layer(decoder3)\n",
        "    return output"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krULYziTgLdF"
      },
      "source": [
        "data_path = 'Data/'\n",
        "data = pd.read_csv(data_path+'Preprocessed_data.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJx3ZfRiidSF",
        "outputId": "2dcb841d-bf98-4eed-eaef-d7ddbf5575a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "movies_list = data['Movie_Id'].unique()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "789"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTWkvDaufUAZ",
        "outputId": "3fbafb6e-86e6-432f-ba55-74cdfb5c9b3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Remove movies with ratings less than 100\n",
        "movies_counter = {}\n",
        "for i in range(df.shape[0]):\n",
        "    movies_counter.setdefault(df.iloc[i]['Movie_Id'],0)\n",
        "    movies_counter[df.iloc[i]['Movie_Id']] += 1\n",
        "    \n",
        "del_movies = [x for x in movies_counter if movies_counter[x] < 100]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0s4WI9jflMT"
      },
      "source": [
        "movies_list = [x for x in movies_list if x not in del_movies]\n",
        "movies_list = dict([(movies_list[x],x) for x in range(len(movies_list))])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "814Uv31GiecV",
        "outputId": "59156d65-40fb-4519-87f9-74bedf088288",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "userData = {}\n",
        "for i in range(data.shape[0]):\n",
        "  user = int(data.iloc[i]['Cust_Id'])\n",
        "  userData.setdefault(user,[0 for x in movies_list])\n",
        "  if data.iloc[i]['Movie_Id'] in movies_list:\n",
        "    userData[user][movies_list[data.iloc[i]['Movie_Id']]-1] = int(data.iloc[i]['Rating'])\n",
        "\n",
        "len(userData)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "393393"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs2HEev7oKyz"
      },
      "source": [
        "def format_time(elapsed):\n",
        "  elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "  # Format as hh:mm:ss\n",
        "  return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2fn-U5pVgXN"
      },
      "source": [
        "# Train model\n",
        "def trainModel(X_train,X_test,model,optimizer,lossFunc,weights,batch_size=64,num_epochs=10,lr=0.01,num_re_feeding=1):\n",
        "  loss_values = []\n",
        "   \n",
        "  for epoch_i in range(0, num_epochs):\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_epochs))\n",
        "      print('Training...')\n",
        "      t0 = time.time()\n",
        "      total_loss = 0\n",
        "      model.train(True)\n",
        "      for step in range(math.ceil(X_train.shape[0]/batch_size)):\n",
        "          if step % 500 == 0 and not step == 0:\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              print('  Batch {:>5,}   Elapsed: {:}.'.format(step, elapsed))\n",
        "          \n",
        "          b_inputs = X_train[step*batch_size:(step*batch_size)+batch_size].float().to(device)\n",
        "          b_weights = weights[step*batch_size:(step*batch_size)+batch_size].float().to(device)\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(features = b_inputs)\n",
        "          loss,num_ratings = lossFunc(outputs,b_inputs,b_weights)\n",
        "          loss = torch.sum(loss)/num_ratings\n",
        "          total_loss += loss.item()\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          for i in range(num_re_feeding):\n",
        "            b_inputs = Variable(outputs.data)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features = b_inputs)\n",
        "            loss,num_ratings = lossFunc(outputs,b_inputs,b_weights)\n",
        "            \n",
        "            loss = torch.sum(loss)/num_ratings\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "      \n",
        "      avg_train_loss = total_loss / math.ceil(X_train.shape[0]/batch_size)\n",
        "      loss_values.append(avg_train_loss)\n",
        "      print(\"\")\n",
        "      print(\"  Total MMSE: {0:.2f}\".format(total_loss))\n",
        "      print(\"  Average MMSE: {0:.2f}\".format(avg_train_loss))\n",
        "      print(\"  Average RMSE: {0:.2f}\".format(math.sqrt(avg_train_loss)))\n",
        "      print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "      t0 = time.time()\n",
        "      model.eval()\n",
        "      test_loss = 0\n",
        "      for validationStep in range(math.ceil(X_test.shape[0]/batch_size)):\n",
        "          # Add batch to GPU\n",
        "          b_inputs = X_test[validationStep*batch_size:(validationStep*batch_size)+batch_size].float().to(device)\n",
        "          b_weights = torch.ones(b_inputs.size()[0],1).to(device)\n",
        "          \n",
        "          with torch.no_grad():\n",
        "              outputs = model(features = b_inputs)\n",
        "              #print(outputs)\n",
        "          \n",
        "          loss,num_ratings = lossFunc(outputs,b_inputs,b_weights)\n",
        "          loss = torch.sqrt(torch.sum(loss)/num_ratings)\n",
        "          test_loss += loss.item()\n",
        "      avg_test_loss = test_loss / math.ceil(X_test.shape[0]/batch_size)\n",
        "      print(\"\")\n",
        "      print(\"  Average test RMSE: {0:.2f}\".format(avg_test_loss))\n",
        "      print(\"  Validation epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "  return model, loss_values"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0dprMY9eret"
      },
      "source": [
        "# Get error for computing Adaboost weights\n",
        "def getError(X_train,model,lossFunc,weights,batch_size=64):\n",
        "  loss_values = []\n",
        "  weighted_loss = []\n",
        "  model.eval()\n",
        "  num_ratings = 0\n",
        "  for step in range(math.ceil(X_train.shape[0]/batch_size)):\n",
        "      if step % 500 == 0 and not step == 0:\n",
        "          \n",
        "          print('  Batch {:>5,}'.format(step))\n",
        "      \n",
        "      b_inputs = X_train[step*batch_size:(step*batch_size)+batch_size].float().to(device)\n",
        "      b_weights = weights[step*batch_size:(step*batch_size)+batch_size].float().to(device)\n",
        "      b_ones = torch.ones(b_inputs.size()[0],1).to(device)\n",
        "      with torch.no_grad():\n",
        "        outputs = model(features = b_inputs)\n",
        "      weighted,ratings = lossFunc(outputs,b_inputs,b_weights)\n",
        "      num_ratings += ratings.item()\n",
        "      weighted = torch.sum(weighted,dim=1,keepdims=True)\n",
        "      weighted_loss.extend([np.sqrt(weighted[x][0].item()) for x in range(weighted.size()[0])])\n",
        "      loss,_ = lossFunc(outputs,b_inputs,b_weights)\n",
        "      loss = torch.sum(loss,dim=1,keepdims=True)\n",
        "      loss_values.extend([np.sqrt(loss[x][0].item()) for x in range(loss.size()[0])])\n",
        "  \n",
        "  return loss_values,weighted_loss,num_ratings"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ5FSy-pHAEf"
      },
      "source": [
        "# Custom loss function\n",
        "def MMSE(y_pred,y,weights):\n",
        "  masks = y != 0\n",
        "  num_ratings = torch.sum(masks.float())\n",
        "  return weights*masks.float()*torch.square(y_pred - y),num_ratings"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i92n1EvVEre-",
        "outputId": "277ba501-06a6-4914-a917-b7cddc5bda18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "samples = []\n",
        "for sample in userData:\n",
        "  num_ratings = len([1 for x in userData[sample] if x != 0])\n",
        "  if num_ratings >= 5:\n",
        "    samples.append(userData[sample])\n",
        "\n",
        "len(samples)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "222828"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dloiTINJ5-W",
        "outputId": "38482fa5-20c2-46b7-bf8c-6d8466624f67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "userData = np.array(samples)\n",
        "userData.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(222828, 744)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kY_0Tize-LZ",
        "outputId": "ebf68373-5267-4a6d-9b2e-9f78fdcf4d01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train,X_test,_,_ = train_test_split(userData,userData,test_size=0.2,random_state=42)\n",
        "\n",
        "X_train.shape,X_test.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((178262, 744), (44566, 744))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKU9BuDdg85O",
        "outputId": "7ba6bd78-6655-4d78-c3a7-a8efb6810424",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    # Use GPU if available\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG4QoQxI7bcf"
      },
      "source": [
        "# Without Adaboost\n",
        "N = X_train.shape[0]\n",
        "weights = np.ones((N,1))\n",
        "model = AutoEncoder(input_shape=X_train.shape[1])\n",
        "model.cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n",
        "\n",
        "X_train = torch.tensor(X_train)\n",
        "X_test = torch.tensor(X_test)\n",
        "weights = torch.tensor(weights)\n",
        "model,loss_values = trainModel(X_train,X_test,model,optimizer,MMSE,weights=weights,num_epochs=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoYYyzeLjVYo"
      },
      "source": [
        "test_samples = X_test.detach().cpu().numpy()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8wh9M5_I01s"
      },
      "source": [
        "# Testing\n",
        "k = 2\n",
        "model.eval()\n",
        "results = []\n",
        "for sample in test_samples:\n",
        "  ratings = [(x,sample[x]) for x in range(len(sample)) if sample[x] > 0]\n",
        "  sampled_ratings = random.sample(ratings,k)\n",
        "  \n",
        "  for i in sampled_ratings:\n",
        "    sample[i[0]] = 0\n",
        "  with torch.no_grad():\n",
        "    output = model(torch.tensor(sample).float().to(device))\n",
        "  predictions = output.cpu().numpy()\n",
        "  predictions = np.array([predictions[x[0]] for x in sampled_ratings])\n",
        "  actual = np.array([x[1] for x in sampled_ratings])\n",
        "  rmse = np.sqrt(np.mean((predictions-actual)**2))\n",
        "  results.append(rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL7LOkLfKqy4"
      },
      "source": [
        "np.mean(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dapz4exFhA9K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}